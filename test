# Updated MVP Prediction Code

import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, GridSearchCV

# Load Data
stats = pd.read_csv("player_mvp_stats.csv")

# Handle Missing Values
stats.fillna(stats.mean(), inplace=True)

# Define Predictors
predictors = ["Age", "G_y", "GS", "MP", "FG", "FGA", 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'W', 'L', 'W/L%', 'PS/G', 'PA/G', 'SRS']

# Split Data
train = stats[stats["Year"] < 2024]
test = stats[stats["Year"] == 2024]

# Feature Scaling
scaler = StandardScaler()
train[predictors] = scaler.fit_transform(train[predictors])
test[predictors] = scaler.transform(test[predictors])

# Ridge Regression Model
ridge = Ridge(alpha=0.1)

# Hyperparameter Tuning for Ridge
ridge_params = {'alpha': [0.01, 0.1, 1, 10]}
ridge_grid = GridSearchCV(ridge, ridge_params, cv=5)
ridge_grid.fit(train[predictors], train["Share"])

# Best Model
ridge_best = ridge_grid.best_estimator_

# Predictions
predictions = ridge_best.predict(test[predictors])
predictions = pd.DataFrame(predictions, columns=["predictions"], index=test.index)

# Combine and Evaluate
combination = pd.concat([test[["Player", "Share"]], predictions], axis=1)
print("MSE:", mean_squared_error(combination["Share"], combination["predictions"]))
print("R²:", r2_score(combination["Share"], combination["predictions"]))
print("MAE:", mean_absolute_error(combination["Share"], combination["predictions"]))

# RandomForest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=1)

# Hyperparameter Tuning for RandomForest
rf_params = {'n_estimators': [50, 100, 200], 'min_samples_split': [2, 5, 10]}
rf_grid = GridSearchCV(rf, rf_params, cv=5)
rf_grid.fit(train[predictors], train["Share"])

# Best Model
rf_best = rf_grid.best_estimator_

# Predictions
rf_predictions = rf_best.predict(test[predictors])
rf_predictions = pd.DataFrame(rf_predictions, columns=["predictions"], index=test.index)

# Combine and Evaluate
rf_combination = pd.concat([test[["Player", "Share"]], rf_predictions], axis=1)
print("RandomForest MSE:", mean_squared_error(rf_combination["Share"], rf_combination["predictions"]))
print("RandomForest R²:", r2_score(rf_combination["Share"], rf_combination["predictions"]))
print("RandomForest MAE:", mean_absolute_error(rf_combination["Share"], rf_combination["predictions"]))

# Additional Evaluation Metrics
def calculate_metrics(combination):
    print("Top 5 Actual MVPs:")
    print(combination.sort_values("Share", ascending=False).head(5))
    print("Top 5 Predicted MVPs:")
    print(combination.sort_values("predictions", ascending=False).head(5))

calculate_metrics(combination)
calculate_metrics(rf_combination)

# Alternative Model for Predicting Player Rankings
train["Rank"] = train["Share"].rank(ascending=False)
test["Rank"] = test["Share"].rank(ascending=False)

ridge_best.fit(train[predictors], train["Rank"])
rank_predictions = ridge_best.predict(test[predictors])
rank_combination = pd.concat([test[["Player", "Rank"]], pd.DataFrame(rank_predictions, columns=["predicted_rank"], index=test.index)], axis=1)

print("Player Ranking Evaluation:")
calculate_metrics(rank_combination)

# Save Model
import joblib
joblib.dump(ridge_best, 'ridge_best_model.pkl')
joblib.dump(rf_best, 'rf_best_model.pkl')
